import torch
from transformers import BertTokenizer, BertForSequenceClassification
import torch.nn.functional as F
from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration
import requests
from bs4 import BeautifulSoup
import re

# KoBART ìš”ì•½ ëª¨ë¸
bart_tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')
bart_model = BartForConditionalGeneration.from_pretrained('gogamza/kobart-base-v2')

# BERT ê°ì„± ë¶„ì„ ëª¨ë¸
bert_tokenizer = BertTokenizer.from_pretrained("kykim/bert-kor-base")
bert_model = BertForSequenceClassification.from_pretrained("kykim/bert-kor-base", num_labels=2)

# ìš”ì•½ í•¨ìˆ˜
def summarize(text):
    text = re.sub(r'(.)\1{2,}', r'\1', text.strip())
    inputs = bart_tokenizer([text], max_length=1024, return_tensors='pt', truncation=True)
    summary_ids = bart_model.generate(
        inputs['input_ids'],
        max_length=100,
        num_beams=4,
        no_repeat_ngram_size=3,
        repetition_penalty=2.0,
        early_stopping=True
    )
    return bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True)

# ê°ì„± ë¶„ì„ í•¨ìˆ˜
def classify_emotion(text):
    tokens = bert_tokenizer(text, padding=True, truncation=True, return_tensors="pt")
    with torch.no_grad():
        prediction = bert_model(**tokens)
    prediction = F.softmax(prediction.logits, dim=1)
    output = prediction.argmax(dim=1).item()
    labels = ["ë¶€ì •ì ", "ê¸ì •ì "]
    return labels[output]

# HTML ê°€ì ¸ì˜¤ê¸°
def fetch_html(url):
    headers = {"User-Agent": "Mozilla/5.0"}
    response = requests.get(url, headers=headers)
    response.raise_for_status()
    return BeautifulSoup(response.text, "html.parser")

# ë‰´ìŠ¤ íŒŒì‹±
def parse_naver(soup):
    title = soup.select_one('h2#title_area') or soup.select_one('h3#articleTitle')
    title = title.get_text(strip=True) if title else ""
    time = soup.select_one('span._ARTICLE_DATE_TIME')
    time = time.get_text(strip=True) if time else ""
    content = soup.select_one('article') or soup.select_one('#articleBodyContents')
    content = content.get_text(separator=' ', strip=True) if content else ""
    return title, time, content

def parse_daum(soup):
    title = soup.select_one('h3.tit_view')
    title = title.get_text(strip=True) if title else ""
    time = soup.select_one('span.num_date')
    time = time.get_text(strip=True) if time else ""
    content = soup.select_one('section p')
    content = content.get_text(separator=' ', strip=True) if content else ""
    return title, time, content

# ì „ì²´ ì‹¤í–‰
def run_news_summary_and_emotion():
    url = input("ë‰´ìŠ¤ ê¸°ì‚¬ URLì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
    try:
        soup = fetch_html(url)

        if "naver.com" in url:
            title, time, content = parse_naver(soup)
            print("[ë„¤ì´ë²„ ë‰´ìŠ¤]")
        elif "daum.net" in url:
            title, time, content = parse_daum(soup)
            print("[ë‹¤ìŒ ë‰´ìŠ¤]")
        else:
            print("âš  í˜„ì¬ëŠ” ë„¤ì´ë²„ì™€ ë‹¤ìŒ ë‰´ìŠ¤ë§Œ ì§€ì›í•©ë‹ˆë‹¤.")
            return

        summary = summarize(content)
        sentiment = classify_emotion(summary)

        print(f"\nì œëª©: {title}")
        print(f"ì‘ì„± ì‹œê°„: {time}")
        print(f"\nğŸ“„ ê¸°ì‚¬ ìš”ì•½:\n{summary}")
        print(f"\nğŸ§  ê°ì„± ë¶„ì„ ê²°ê³¼: {sentiment}")

    except requests.exceptions.RequestException as e:
        print("ìš”ì²­ ì˜¤ë¥˜:", e)

# ì‹¤í–‰
run_news_summary_and_emotion()
